{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "src_path = r\"C:\\Users\\asrit\\Documents\\516-Project-main\\516-Project-main\\src\"\n",
    "if src_path not in sys.path:\n",
    "    sys.path.append(src_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:No module named 'inFairness': SenSeI and SenSR will be unavailable. To install, run:\n",
      "pip install 'aif360[inFairness]'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import shap\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error, accuracy_score\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "from data_loader import load_dataset\n",
    "from modeling import train_smote_forest, evaluate_model, train_random_forest\n",
    "from fairness_metrics import print_group_rates, disparate_impact, equal_opportunity\n",
    "from preprocess import apply_reweighing\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from fairlearn.postprocessing import ThresholdOptimizer\n",
    "from fairlearn.metrics import MetricFrame, selection_rate\n",
    "from fairlearn.reductions import ExponentiatedGradient, EqualizedOdds\n",
    "\n",
    "from lime.lime_tabular import LimeTabularExplainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "df = load_dataset(\"../data/cleaned_dataset.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a preprocessing mitigation strategy - AIF360â€™s Reweighing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Man' 'Woman' 'NonBinary']\n",
      "object\n",
      "['<35' '>35']\n",
      "object\n",
      "Index(['Gender_encoded', 'Age_encoded', 'YearsCode', 'YearsCodePro',\n",
      "       'ComputerSkills', 'PreviousSalary', 'Gender_group_NonBinary',\n",
      "       'Gender_group_Woman', 'Age_group_>35', 'EdLevel_NoHigherEd',\n",
      "       'EdLevel_Other', 'EdLevel_PhD', 'EdLevel_Undergraduate'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Apply AIF360 Reweighing (only used for fairness metrics, not for training if SMOTE is applied)\n",
    "df = apply_reweighing(df, protected_attr='Gender', label_col='Employment')\n",
    "\n",
    "gender_map = {0: 'Man', 1: 'NonBinary', 2: 'Woman'}\n",
    "#age_map = {0: '<35', 1: '>35'}\n",
    "\n",
    "df['Gender_group'] = df['Gender'].map(gender_map)\n",
    "df['Age_group'] = df['Age']  \n",
    "\n",
    "\n",
    "print(df['Gender_group'].unique())\n",
    "print(df['Gender_group'].dtype)\n",
    "\n",
    "print(df['Age_group'].unique())\n",
    "print(df['Age_group'].dtype)\n",
    "\n",
    "\n",
    "# Step 2: Feature engineering for modeling\n",
    "features = [\n",
    "    'Gender_group', 'Age_group', 'EdLevel', 'Gender_encoded', 'Age_encoded',\n",
    "    'YearsCode', 'YearsCodePro', 'ComputerSkills', 'PreviousSalary'\n",
    "]\n",
    "X = pd.get_dummies(df[features], drop_first=True)\n",
    "y = df['Employment'].astype(int)\n",
    "\n",
    "print(X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8051181995553337\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.18      0.19      0.18      2571\n",
      "           1       0.89      0.89      0.89     19468\n",
      "\n",
      "    accuracy                           0.81     22039\n",
      "   macro avg       0.54      0.54      0.54     22039\n",
      "weighted avg       0.81      0.81      0.81     22039\n",
      "\n",
      "Pearson Correlation: 0.0743\n",
      "Mean Absolute Error: 0.1949\n",
      "Accuracy for Gender Man: 0.8001\n",
      "Accuracy for Gender NonBinary: 0.8318\n",
      "Accuracy for Gender Woman: 0.8947\n",
      "Equal Accuracy Gap: 0.0946\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Choose model strategy\n",
    "use_smote = True  # Toggle this to False to use reweighing instead of SMOTE\n",
    "\n",
    "if use_smote:\n",
    "    model, X_test, y_test, y_pred = train_smote_forest(X, y)\n",
    "else:\n",
    "    sample_weights = df['instance_weight']\n",
    "    model, X_test, y_test, y_pred = train_random_forest(X, y, sample_weights)\n",
    "\n",
    "# Step 4: Evaluate base model performance (before fairness post-processing)\n",
    "evaluate_model(y_test, y_pred)\n",
    "\n",
    "# --- 3. Equal Accuracy by Gender Group ---\n",
    "y_test_aligned = y_test.reset_index(drop=True)\n",
    "y_pred_aligned = pd.Series(y_pred).reset_index(drop=True)\n",
    "gender_column = df.loc[y_test.index, 'Gender_group'].reset_index(drop=True)\n",
    "\n",
    "# --- 1. Pearson's Correlation ---\n",
    "pearson_corr, _ = pearsonr(y_test, y_pred)\n",
    "print(f\"Pearson Correlation: {round(pearson_corr, 4)}\")\n",
    "\n",
    "# --- 2. Mean Absolute Error ---\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "print(f\"Mean Absolute Error: {round(mae, 4)}\")\n",
    "\n",
    "equal_accuracy = {}\n",
    "for group in sorted(gender_column.unique()):\n",
    "    group_mask = gender_column == group\n",
    "    acc = accuracy_score(y_test_aligned[group_mask], y_pred_aligned[group_mask])\n",
    "    equal_accuracy[group] = acc\n",
    "    print(f\"Accuracy for Gender {group}: {round(acc, 4)}\")\n",
    "\n",
    "equal_accuracy_gap = max(equal_accuracy.values()) - min(equal_accuracy.values())\n",
    "print(f\"Equal Accuracy Gap: {round(equal_accuracy_gap, 4)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Selection Rates by Gender:\n",
      "0: 0.88\n",
      "2: 0.91\n",
      "1: 0.87\n",
      "\n",
      "Selection Rates by Age:\n",
      "<35: 0.90\n",
      ">35: 0.85\n",
      "\n",
      "Selection Rates by EdLevel:\n",
      "Master: 0.88\n",
      "Undergraduate: 0.90\n",
      "PhD: 0.90\n",
      "Other: 0.84\n",
      "NoHigherEd: 0.80\n",
      "\n",
      "Disparate Impact (2/0): 1.03\n",
      "\n",
      "Disparate Impact (1/0): 0.98\n",
      "\n",
      "Disparate Impact (>35/<35): 0.95\n",
      "\n",
      "Equal Opportunity by group:\n",
      "0: TPR = 0.89\n",
      "1: TPR = 0.89\n",
      "2: TPR = 0.87\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Custom fairness metrics based on fairness-aware predictions\n",
    "print_group_rates(df, 'Gender')\n",
    "print_group_rates(df, 'Age')\n",
    "print_group_rates(df, 'EdLevel')\n",
    "\n",
    "# Disparate impact comparisons\n",
    "disparate_impact(df, 2, 0, 'Gender')  # Woman vs Man\n",
    "disparate_impact(df, 1, 0, 'Gender')  # NonBinary vs Man\n",
    "disparate_impact(df, '>35', '<35', 'Age')\n",
    "\n",
    "# Equal opportunity using fairness-aware predictions\n",
    "equal_opportunity(\n",
    "    y_test.reset_index(drop=True),\n",
    "    pd.Series(y_pred), \n",
    "    [0, 1, 2],  # Man, NonBinary, Woman\n",
    "    'Gender',\n",
    "    df.reset_index(drop=True)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_consistent_metrics(y_test, y_pred, sensitive_col='Gender_encoded', label_name='Gender'):\n",
    "    \"\"\"\n",
    "    Evaluate predictions using MAE, Pearson correlation, and group-wise accuracy.\n",
    "    Assumes access to global df and X_test_pmute for index alignment.\n",
    "    \"\"\"\n",
    "    # Align predictions and labels\n",
    "    y_test = y_test.reset_index(drop=True)\n",
    "    y_pred = pd.Series(y_pred).reset_index(drop=True)\n",
    "    \n",
    "    # Align sensitive attribute values from df using test indices\n",
    "    sensitive_series = df.loc[y_test.index, sensitive_col].reset_index(drop=True)\n",
    "\n",
    "    # --- 1. Pearson Correlation ---\n",
    "    pearson_corr, _ = pearsonr(y_test, y_pred)\n",
    "    print(f\"\\nPearson Correlation: {round(pearson_corr, 4)}\")\n",
    "\n",
    "    # --- 2. Mean Absolute Error ---\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    print(f\"Mean Absolute Error: {round(mae, 4)}\")\n",
    "\n",
    "    # --- 3. Group-wise Accuracy ---\n",
    "    print(f\"\\nAccuracy by {label_name} group:\")\n",
    "    for group in sorted(sensitive_series.unique()):\n",
    "        group_mask = sensitive_series == group\n",
    "        acc = accuracy_score(y_test[group_mask], y_pred[group_mask])\n",
    "        print(f\"{label_name} {group}: {round(acc, 4)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fairlearn.reductions import ExponentiatedGradient, EqualizedOdds\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from fairlearn.metrics import MetricFrame, selection_rate\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# --- 1. Split original train and test sets from preprocessed X, y ---\n",
    "X_train_orig = X.loc[X.index.difference(X_test.index)].copy()\n",
    "y_train_orig = y.loc[X.index.difference(X_test.index)].copy()\n",
    "X_test_eg = X_test.copy()\n",
    "y_test_eg = y_test.copy()\n",
    "\n",
    "# --- 2. Add Gender_group (protected attr) as column to X ---\n",
    "X_aug = X_train_orig.copy()\n",
    "X_aug[\"Gender_group\"] = df.loc[X_train_orig.index, \"Gender_group\"].values\n",
    "\n",
    "# --- 3. Combine X + y + protected attr into one DataFrame ---\n",
    "train_combined = X_aug.copy()\n",
    "train_combined[\"Employment\"] = y_train_orig.values\n",
    "\n",
    "# --- 4. One-hot encode Gender_group to allow SMOTE ---\n",
    "train_encoded = pd.get_dummies(train_combined, columns=[\"Gender_group\"])\n",
    "\n",
    "# --- 5. Apply SMOTE on features (excluding label) ---\n",
    "smote = SMOTE(random_state=42)\n",
    "X_smoted, y_resampled = smote.fit_resample(\n",
    "    train_encoded.drop(columns=[\"Employment\"]),\n",
    "    train_encoded[\"Employment\"]\n",
    ")\n",
    "\n",
    "# --- 6. Recover protected attribute from one-hot encoded columns ---\n",
    "gender_cols = [col for col in X_smoted.columns if col.startswith(\"Gender_group_\")]\n",
    "sensitive_feature_train = X_smoted[gender_cols].idxmax(axis=1).str.replace(\"Gender_group_\", \"\")\n",
    "\n",
    "# --- 7. Drop one-hot protected attr columns from training data ---\n",
    "X_resampled_final = X_smoted.drop(columns=gender_cols)\n",
    "\n",
    "# --- 8. Train fairness-aware model ---\n",
    "base_model = LogisticRegression(solver=\"liblinear\", class_weight='balanced')\n",
    "eg_model = ExponentiatedGradient(\n",
    "    estimator=base_model,\n",
    "    constraints=EqualizedOdds(),\n",
    "    eps=0.01\n",
    ")\n",
    "eg_model.fit(X_resampled_final, y_resampled, sensitive_features=sensitive_feature_train)\n",
    "\n",
    "# --- 9. Drop protected one-hot columns from test set to match training features ---\n",
    "X_test_eg = X_test_eg.drop(columns=[col for col in X_test_eg.columns if col.startswith(\"Gender_group_\")])\n",
    "sensitive_feature_test = df.loc[X_test_eg.index, \"Gender_group\"]\n",
    "y_pred_eg = eg_model.predict(X_test_eg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exponentiated Gradient with SMOTE and Class Weights:\n",
      "Accuracy: 0.7763963882208812\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.18      0.25      0.21      2571\n",
      "           1       0.90      0.85      0.87     19468\n",
      "\n",
      "    accuracy                           0.78     22039\n",
      "   macro avg       0.54      0.55      0.54     22039\n",
      "weighted avg       0.81      0.78      0.79     22039\n",
      "\n",
      "Accuracy: 0.7763963882208812\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.18      0.25      0.21      2571\n",
      "           1       0.90      0.85      0.87     19468\n",
      "\n",
      "    accuracy                           0.78     22039\n",
      "   macro avg       0.54      0.55      0.54     22039\n",
      "weighted avg       0.81      0.78      0.79     22039\n",
      "\n",
      "\n",
      "Pearson Correlation: 0.0854\n",
      "Mean Absolute Error: 0.2236\n",
      "\n",
      "Accuracy by Gender group:\n",
      "Gender Man: 0.7757\n",
      "Gender NonBinary: 0.7766\n",
      "Gender Woman: 0.7922\n",
      "\n",
      "Pearson Correlation: 0.0854\n",
      "Mean Absolute Error: 0.2236\n",
      "\n",
      "Accuracy by Age group:\n",
      "Age <35: 0.7805\n",
      "Age >35: 0.7679\n",
      "\n",
      "Pearson Correlation: 0.0854\n",
      "Mean Absolute Error: 0.2236\n",
      "\n",
      "Accuracy by EdLevel group:\n",
      "EdLevel Master: 0.7842\n",
      "EdLevel NoHigherEd: 0.7808\n",
      "EdLevel Other: 0.7811\n",
      "EdLevel PhD: 0.761\n",
      "EdLevel Undergraduate: 0.7716\n"
     ]
    }
   ],
   "source": [
    "# --- 10. Performance Metrics ---\n",
    "print(\"Exponentiated Gradient with SMOTE and Class Weights:\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test_eg, y_pred_eg))\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test_eg, y_pred_eg))\n",
    "\n",
    "# --- 11. Utility Evaluation ---\n",
    "evaluate_model(y_test_eg, y_pred_eg)\n",
    "run_consistent_metrics(y_test_eg, y_pred_eg, sensitive_col='Gender_group', label_name='Gender')\n",
    "run_consistent_metrics(y_test_eg, y_pred_eg, sensitive_col='Age_group', label_name='Age')\n",
    "run_consistent_metrics(y_test_eg, y_pred_eg, sensitive_col='EdLevel', label_name='EdLevel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fairness metrics by Gender:\n",
      "              accuracy  selection_rate\n",
      "Gender_group                          \n",
      "Man           0.777039        0.833714\n",
      "NonBinary     0.739336        0.815166\n",
      "Woman         0.778752        0.845029\n",
      "\n",
      "Fairness metrics by Age:\n",
      "           accuracy  selection_rate\n",
      "Age_group                          \n",
      "<35        0.841863        0.913446\n",
      ">35        0.653946        0.685074\n",
      "\n",
      "Fairness metrics by EdLevel:\n",
      "               accuracy  selection_rate\n",
      "EdLevel                                \n",
      "Master         0.875536        0.993205\n",
      "NoHigherEd     0.291324        0.155251\n",
      "Other          0.535869        0.520099\n",
      "PhD            0.845269        0.916880\n",
      "Undergraduate  0.838215        0.904640\n"
     ]
    }
   ],
   "source": [
    "# --- 12. MetricFrame Fairness ---\n",
    "for attr, label in [(\"Gender_group\", \"Gender\"), (\"Age_group\", \"Age\"), (\"EdLevel\", \"EdLevel\")]:\n",
    "    mf = MetricFrame(\n",
    "        metrics={\"accuracy\": accuracy_score, \"selection_rate\": selection_rate},\n",
    "        y_true=y_test_eg,\n",
    "        y_pred=y_pred_eg,\n",
    "        sensitive_features=df.loc[X_test_eg.index, attr]\n",
    "    )\n",
    "    print(f\"\\nFairness metrics by {label}:\")\n",
    "    print(mf.by_group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Selection Rates by Gender:\n",
      "0: 0.88\n",
      "2: 0.91\n",
      "1: 0.87\n",
      "\n",
      "Selection Rates by Age:\n",
      "<35: 0.90\n",
      ">35: 0.85\n",
      "\n",
      "Selection Rates by EdLevel:\n",
      "Master: 0.88\n",
      "Undergraduate: 0.90\n",
      "PhD: 0.90\n",
      "Other: 0.84\n",
      "NoHigherEd: 0.80\n",
      "\n",
      "Disparate Impact (Woman/Man): 1.03\n",
      "\n",
      "Disparate Impact (NonBinary/Man): 0.98\n",
      "\n",
      "Disparate Impact (>35/<35): 0.95\n",
      "\n",
      "Disparate Impact (NoHigherEd/Master): 0.91\n",
      "\n",
      "Disparate Impact (PhD/Undergraduate): 0.99\n",
      "\n",
      "Disparate Impact (Other/Undergraduate): 0.93\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9285316081807473"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- 13. Custom Fairness Metrics ---\n",
    "print_group_rates(df, 'Gender')\n",
    "print_group_rates(df, 'Age')\n",
    "print_group_rates(df, 'EdLevel')\n",
    "\n",
    "# gender\n",
    "disparate_impact(df, 'Woman', 'Man', 'Gender_group')      \n",
    "disparate_impact(df, 'NonBinary', 'Man', 'Gender_group') \n",
    "\n",
    "# age\n",
    "disparate_impact(df, '>35', '<35', 'Age_group')             \n",
    "\n",
    "# Education level\n",
    "disparate_impact(df, 'NoHigherEd', 'Master', 'EdLevel')\n",
    "disparate_impact(df, 'PhD', 'Undergraduate', 'EdLevel')\n",
    "disparate_impact(df, 'Other', 'Undergraduate', 'EdLevel')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Equal Opportunity by group:\n",
      "Man: TPR = 0.85\n",
      "NonBinary: TPR = 0.83\n",
      "Woman: TPR = 0.85\n",
      "\n",
      "Equal Opportunity by group:\n",
      "<35: TPR = 0.85\n",
      ">35: TPR = 0.84\n",
      "\n",
      "Equal Opportunity by group:\n",
      "Undergraduate: TPR = 0.84\n",
      "Master: TPR = 0.85\n",
      "PhD: TPR = 0.85\n",
      "Other: TPR = 0.85\n",
      "NoHigherEd: TPR = 0.84\n"
     ]
    }
   ],
   "source": [
    "equal_opportunity(\n",
    "    y_test_eg.reset_index(drop=True),\n",
    "    pd.Series(y_pred_eg),\n",
    "    ['Man', 'NonBinary', 'Woman'],\n",
    "    'Gender_group',\n",
    "    df.reset_index(drop=True)\n",
    ")\n",
    "\n",
    "df.loc[X_test_eg.index, \"predicted_label\"] = y_pred_eg\n",
    "\n",
    "equal_opportunity(\n",
    "    y_test_eg.reset_index(drop=True),\n",
    "    pd.Series(y_pred_eg),\n",
    "    ['<35', '>35'],             # distinct values in Age_group\n",
    "    'Age_group',\n",
    "    df.reset_index(drop=True)\n",
    ")\n",
    "\n",
    "equal_opportunity(\n",
    "    y_test_eg.reset_index(drop=True),\n",
    "    pd.Series(y_pred_eg),\n",
    "    ['Undergraduate', 'Master', 'PhD', 'Other', 'NoHigherEd'],   # use actual values in your data\n",
    "    'EdLevel',\n",
    "    df.reset_index(drop=True)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Post Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Gender_group</th>\n",
       "      <th>Age_group</th>\n",
       "      <th>EdLevel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Man</td>\n",
       "      <td>&lt;35</td>\n",
       "      <td>Master</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Man</td>\n",
       "      <td>&lt;35</td>\n",
       "      <td>Undergraduate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Man</td>\n",
       "      <td>&lt;35</td>\n",
       "      <td>Master</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Man</td>\n",
       "      <td>&lt;35</td>\n",
       "      <td>Undergraduate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Man</td>\n",
       "      <td>&gt;35</td>\n",
       "      <td>PhD</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Gender_group Age_group        EdLevel\n",
       "0          Man       <35         Master\n",
       "1          Man       <35  Undergraduate\n",
       "2          Man       <35         Master\n",
       "3          Man       <35  Undergraduate\n",
       "4          Man       >35            PhD"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[['Gender_group', 'Age_group', 'EdLevel']].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Fairness-aware Evaluation (SMOTE + Equalized Odds):\n",
      "Accuracy: 0.8660556286582876\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.23      0.06      0.10      2576\n",
      "           1       0.89      0.97      0.93     19463\n",
      "\n",
      "    accuracy                           0.87     22039\n",
      "   macro avg       0.56      0.52      0.51     22039\n",
      "weighted avg       0.81      0.87      0.83     22039\n",
      "\n",
      "Pearson Correlation: 0.0635\n",
      "Mean Absolute Error: 0.1339\n",
      "\n",
      "Accuracy by Gender group:\n",
      "Gender Man: 0.8649\n",
      "Gender NonBinary: 0.8616\n",
      "Gender Woman: 0.8903\n",
      "\n",
      "Accuracy by Age group:\n",
      "Age <35: 0.8903\n",
      "Age >35: 0.8217\n",
      "\n",
      "Accuracy by EdLevel group:\n",
      "EdLevel Master: 0.874\n",
      "EdLevel NoHigherEd: 0.7847\n",
      "EdLevel Other: 0.7987\n",
      "EdLevel PhD: 0.8794\n",
      "EdLevel Undergraduate: 0.889\n",
      "\n",
      "Fairness metrics by Gender:\n",
      "              accuracy  selection_rate\n",
      "Gender_group                          \n",
      "Man           0.864945        0.968736\n",
      "NonBinary     0.861575        0.966587\n",
      "Woman         0.890304        0.962782\n",
      "\n",
      "Fairness metrics by Age:\n",
      "           accuracy  selection_rate\n",
      "Age_group                          \n",
      "<35        0.890317        0.981111\n",
      ">35        0.821749        0.945242\n",
      "\n",
      "Fairness metrics by EdLevel:\n",
      "               accuracy  selection_rate\n",
      "EdLevel                                \n",
      "Master         0.873960        0.975217\n",
      "NoHigherEd     0.784710        0.936731\n",
      "Other          0.798713        0.927696\n",
      "PhD            0.879423        0.982962\n",
      "Undergraduate  0.888998        0.979065\n"
     ]
    }
   ],
   "source": [
    "# --- FULL POST-PROCESSING BLOCK WITH CONSISTENCY AND METRICS ---\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from fairlearn.postprocessing import ThresholdOptimizer\n",
    "from sklearn.metrics import accuracy_score, classification_report, mean_absolute_error\n",
    "from fairlearn.metrics import MetricFrame, selection_rate\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "# --- 1. 3-Way Split: Train (60%), Validation (10%), Test (30%) ---\n",
    "X_trainval, X_test_post, y_trainval, y_test_post = train_test_split(\n",
    "    X, y, test_size=0.3, stratify=y, random_state=42\n",
    ")\n",
    "X_train_post, X_val_post, y_train_post, y_val_post = train_test_split(\n",
    "    X_trainval, y_trainval, test_size=0.125, stratify=y_trainval, random_state=42\n",
    ")\n",
    "\n",
    "# --- 2. Apply SMOTE to training data ---\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_post_smote, y_train_post_smote = smote.fit_resample(X_train_post, y_train_post)\n",
    "\n",
    "# --- 3. Train classifier on SMOTE-balanced data ---\n",
    "model_post = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "model_post.fit(X_train_post_smote, y_train_post_smote)\n",
    "\n",
    "# --- 4. Fit ThresholdOptimizer using validation set ---\n",
    "sensitive_gender_val = df.loc[X_val_post.index, \"Gender_group\"]\n",
    "postprocessor = ThresholdOptimizer(\n",
    "    estimator=model_post,\n",
    "    constraints=\"equalized_odds\",\n",
    "    predict_method=\"predict_proba\"\n",
    ")\n",
    "postprocessor.fit(X_val_post, y_val_post, sensitive_features=sensitive_gender_val)\n",
    "\n",
    "# --- 5. Predict on test set ---\n",
    "sensitive_gender_test = df.loc[X_test_post.index, \"Gender_group\"]\n",
    "y_pred_fair = postprocessor.predict(X_test_post, sensitive_features=sensitive_gender_test)\n",
    "\n",
    "# --- 6. Utility & Classification Metrics ---\n",
    "print(\"ðŸ“Š Fairness-aware Evaluation (SMOTE + Equalized Odds):\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test_post, y_pred_fair))\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test_post, y_pred_fair))\n",
    "\n",
    "pearson_corr, _ = pearsonr(y_test_post, y_pred_fair)\n",
    "mae = mean_absolute_error(y_test_post, y_pred_fair)\n",
    "print(f\"Pearson Correlation: {round(pearson_corr, 4)}\")\n",
    "print(f\"Mean Absolute Error: {round(mae, 4)}\")\n",
    "\n",
    "# --- 7. Group-wise Accuracy Reporting ---\n",
    "def run_consistent_metrics(y_true, y_pred, sensitive_col, label_name):\n",
    "    print(f\"\\nAccuracy by {label_name} group:\")\n",
    "    groups = df.loc[X_test_post.index, sensitive_col]\n",
    "    y_true_aligned = y_true.reset_index(drop=True)\n",
    "    y_pred_aligned = pd.Series(y_pred).reset_index(drop=True)\n",
    "    for group in sorted(groups.unique()):\n",
    "        mask = (groups == group).reset_index(drop=True)\n",
    "        acc = accuracy_score(y_true_aligned[mask], y_pred_aligned[mask])\n",
    "        print(f\"{label_name} {group}: {round(acc, 4)}\")\n",
    "\n",
    "run_consistent_metrics(y_test_post, y_pred_fair, 'Gender_group', 'Gender')\n",
    "run_consistent_metrics(y_test_post, y_pred_fair, 'Age_group', 'Age')\n",
    "run_consistent_metrics(y_test_post, y_pred_fair, 'EdLevel', 'EdLevel')\n",
    "\n",
    "# --- 8. MetricFrame Fairness Evaluation ---\n",
    "mf_gender = MetricFrame(metrics={\"accuracy\": accuracy_score, \"selection_rate\": selection_rate},\n",
    "                        y_true=y_test_post, y_pred=y_pred_fair,\n",
    "                        sensitive_features=df.loc[X_test_post.index, \"Gender_group\"])\n",
    "mf_age = MetricFrame(metrics={\"accuracy\": accuracy_score, \"selection_rate\": selection_rate},\n",
    "                     y_true=y_test_post, y_pred=y_pred_fair,\n",
    "                     sensitive_features=df.loc[X_test_post.index, \"Age_group\"])\n",
    "mf_edlevel = MetricFrame(metrics={\"accuracy\": accuracy_score, \"selection_rate\": selection_rate},\n",
    "                         y_true=y_test_post, y_pred=y_pred_fair,\n",
    "                         sensitive_features=df.loc[X_test_post.index, \"EdLevel\"])\n",
    "\n",
    "print(\"\\nFairness metrics by Gender:\")\n",
    "print(mf_gender.by_group)\n",
    "print(\"\\nFairness metrics by Age:\")\n",
    "print(mf_age.by_group)\n",
    "print(\"\\nFairness metrics by EdLevel:\")\n",
    "print(mf_edlevel.by_group)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Selection Rates by Gender:\n",
      "Man: 0.97\n",
      "Woman: 0.96\n",
      "NonBinary: 0.97\n",
      "\n",
      "Selection Rates by Age:\n",
      "<35: 0.98\n",
      ">35: 0.95\n",
      "\n",
      "Selection Rates by EdLevel:\n",
      "Master: 0.98\n",
      "Undergraduate: 0.98\n",
      "PhD: 0.98\n",
      "Other: 0.93\n",
      "NoHigherEd: 0.94\n",
      "\n",
      "Disparate Impact (Safe Computation):\n",
      "Disparate Impact (Woman/Man): 0.99\n",
      "Disparate Impact (NonBinary/Man): 1.0\n",
      "Disparate Impact (>35/<35): 0.96\n",
      "Disparate Impact (NoHigherEd/Master): 0.96\n",
      "Disparate Impact (PhD/Undergraduate): 1.0\n",
      "Disparate Impact (Other/Undergraduate): 0.95\n",
      "\n",
      "Equal Opportunity by group (Gender):\n",
      "Man: TPR = 0.97\n",
      "NonBinary: TPR = 0.97\n",
      "Woman: TPR = 0.97\n",
      "\n",
      "Equal Opportunity by group (Age):\n",
      "<35: TPR = 0.98\n",
      ">35: TPR = 0.95\n",
      "\n",
      "Equal Opportunity by group (EdLevel):\n",
      "Master: TPR = 0.98\n",
      "NoHigherEd: TPR = 0.95\n",
      "Other: TPR = 0.93\n",
      "PhD: TPR = 0.99\n",
      "Undergraduate: TPR = 0.98\n"
     ]
    }
   ],
   "source": [
    "# --- Selection Rates by Group ---\n",
    "print(\"\\nSelection Rates by Gender:\")\n",
    "for g in ['Man', 'Woman', 'NonBinary']:\n",
    "    mask = df.loc[X_test_post.index, 'Gender_group'] == g\n",
    "    rate = pd.Series(y_pred_fair, index=X_test_post.index)[mask].mean()\n",
    "    print(f\"{g}: {round(rate, 2)}\")\n",
    "\n",
    "print(\"\\nSelection Rates by Age:\")\n",
    "for g in ['<35', '>35']:\n",
    "    mask = df.loc[X_test_post.index, 'Age_group'] == g\n",
    "    rate = pd.Series(y_pred_fair, index=X_test_post.index)[mask].mean()\n",
    "    print(f\"{g}: {round(rate, 2)}\")\n",
    "\n",
    "print(\"\\nSelection Rates by EdLevel:\")\n",
    "for g in ['Master', 'Undergraduate', 'PhD', 'Other', 'NoHigherEd']:\n",
    "    mask = df.loc[X_test_post.index, 'EdLevel'] == g\n",
    "    rate = pd.Series(y_pred_fair, index=X_test_post.index)[mask].mean()\n",
    "    print(f\"{g}: {round(rate, 2)}\")\n",
    "\n",
    "# --- Disparate Impact ---\n",
    "print(\"\\nDisparate Impact (Safe Computation):\")\n",
    "disparities = {\n",
    "    \"Woman/Man\": ('Woman', 'Man', 'Gender_group'),\n",
    "    \"NonBinary/Man\": ('NonBinary', 'Man', 'Gender_group'),\n",
    "    \">35/<35\": ('>35', '<35', 'Age_group'),\n",
    "    \"NoHigherEd/Master\": ('NoHigherEd', 'Master', 'EdLevel'),\n",
    "    \"PhD/Undergraduate\": ('PhD', 'Undergraduate', 'EdLevel'),\n",
    "    \"Other/Undergraduate\": ('Other', 'Undergraduate', 'EdLevel'),\n",
    "}\n",
    "\n",
    "for label, (grp1, grp2, col) in disparities.items():\n",
    "    mask1 = df.loc[X_test_post.index, col] == grp1\n",
    "    mask2 = df.loc[X_test_post.index, col] == grp2\n",
    "    \n",
    "    rate1 = pd.Series(y_pred_fair, index=X_test_post.index)[mask1].mean()\n",
    "    rate2 = pd.Series(y_pred_fair, index=X_test_post.index)[mask2].mean()\n",
    "    \n",
    "    if rate2 == 0:\n",
    "        di = \"Undefined (division by zero)\"\n",
    "    else:\n",
    "        di = round(rate1 / rate2, 2)\n",
    "    \n",
    "    print(f\"Disparate Impact ({label}): {di}\")\n",
    "\n",
    "# --- Equal Opportunity by Group ---\n",
    "def compute_tpr_by_group(y_true, y_pred, group_series, label_name):\n",
    "    print(f\"\\nEqual Opportunity by group ({label_name}):\")\n",
    "    for group in sorted(group_series.unique()):\n",
    "        mask = (group_series == group).reset_index(drop=True)\n",
    "        true_pos = ((y_true == 1) & (y_pred == 1))[mask].sum()\n",
    "        actual_pos = (y_true == 1)[mask].sum()\n",
    "        tpr = round(true_pos / actual_pos, 2) if actual_pos > 0 else 0.0\n",
    "        print(f\"{group}: TPR = {tpr}\")\n",
    "\n",
    "compute_tpr_by_group(\n",
    "    y_test_post.reset_index(drop=True),\n",
    "    pd.Series(y_pred_fair),\n",
    "    df.loc[X_test_post.index, 'Gender_group'],\n",
    "    'Gender'\n",
    ")\n",
    "\n",
    "compute_tpr_by_group(\n",
    "    y_test_post.reset_index(drop=True),\n",
    "    pd.Series(y_pred_fair),\n",
    "    df.loc[X_test_post.index, 'Age_group'],\n",
    "    'Age'\n",
    ")\n",
    "\n",
    "compute_tpr_by_group(\n",
    "    y_test_post.reset_index(drop=True),\n",
    "    pd.Series(y_pred_fair),\n",
    "    df.loc[X_test_post.index, 'EdLevel'],\n",
    "    'EdLevel'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explainability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a) Proxy Mute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad7d655a85b941d798e6c5951f09c6dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# --- 1. Train-Test Split ---\n",
    "X_train_pmute, X_test_pmute, y_train_pmute, y_test_pmute = train_test_split(\n",
    "    X, y, test_size=0.3, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "# --- 2. Train Base Model (Logistic Regression with Class Weights) ---\n",
    "lr_model_base = LogisticRegression(solver=\"liblinear\", class_weight='balanced')\n",
    "lr_model_base.fit(X_train_pmute, y_train_pmute)\n",
    "\n",
    "# --- 3. SHAP Explainability (KernelExplainer for probability output) ---\n",
    "explainer = shap.KernelExplainer(\n",
    "    lr_model_base.predict_proba,\n",
    "    X_train_pmute.sample(100, random_state=42)\n",
    ")\n",
    "shap_values = explainer.shap_values(X_test_pmute[:100])\n",
    "\n",
    "# --- 4. Mean Absolute SHAP Importance ---\n",
    "mean_abs_shap = np.abs(shap_values[1]).mean(axis=0)\n",
    "shap_summary = pd.DataFrame({\n",
    "    \"feature\": X_train_pmute.columns,\n",
    "    \"mean_abs_shap\": mean_abs_shap\n",
    "}).sort_values(by=\"mean_abs_shap\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ProxyMute (Revised Proxy List):\n",
      "Accuracy: 0.6645038341122556\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.16      0.44      0.23      2576\n",
      "           1       0.90      0.69      0.79     19463\n",
      "\n",
      "    accuracy                           0.66     22039\n",
      "   macro avg       0.53      0.57      0.51     22039\n",
      "weighted avg       0.82      0.66      0.72     22039\n",
      "\n",
      "Pearson Correlation: 0.0904\n",
      "Mean Absolute Error: 0.3355\n"
     ]
    }
   ],
   "source": [
    "# --- 5. Define Proxy Features to Mute (based on SHAP summary) ---\n",
    "proxy_features = ['PreviousSalary', 'EdLevel_Undergraduate', 'ComputerSkills']\n",
    "\n",
    "# --- 6. Muting Proxy Features in Test Set ---\n",
    "X_test_muted = X_test_pmute.copy()\n",
    "for col in proxy_features:\n",
    "    if col in X_test_muted.columns:\n",
    "        X_test_muted[col] = X_test_muted[col].mean()\n",
    "\n",
    "# --- 7. Predict on Muted Test Set ---\n",
    "y_pred_muted = lr_model_base.predict(X_test_muted)\n",
    "\n",
    "# --- 8. Performance Metrics ---\n",
    "print(\"ProxyMute (Revised Proxy List):\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test_pmute, y_pred_muted))\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test_pmute, y_pred_muted))\n",
    "\n",
    "# --- Utility: Pearson + MAE ---\n",
    "pearson_corr, _ = pearsonr(y_test_pmute, y_pred_muted)\n",
    "mae = mean_absolute_error(y_test_pmute, y_pred_muted)\n",
    "print(f\"Pearson Correlation: {round(pearson_corr, 4)}\")\n",
    "print(f\"Mean Absolute Error: {round(mae, 4)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fairness metrics by Gender (ProxyMute SHAP):\n",
      "              accuracy  selection_rate\n",
      "Gender_group                          \n",
      "Man           0.656197        0.669013\n",
      "NonBinary     0.739857        0.749403\n",
      "Woman         0.801175        0.857982\n",
      "\n",
      "Fairness metrics by Age (ProxyMute SHAP):\n",
      "             accuracy  selection_rate\n",
      "Age_encoded                          \n",
      "0            0.873675        0.959553\n",
      "1            0.282508        0.167479\n",
      "\n",
      "Fairness metrics by EdLevel (ProxyMute SHAP):\n",
      "               accuracy  selection_rate\n",
      "EdLevel                                \n",
      "Master         0.663126        0.666313\n",
      "NoHigherEd     0.596661        0.630931\n",
      "Other          0.543199        0.537377\n",
      "PhD            0.595020        0.570118\n",
      "Undergraduate  0.712071        0.739421\n"
     ]
    }
   ],
   "source": [
    "# --- 9. Fairness Evaluation Using MetricFrame ---\n",
    "# Gender\n",
    "mf_shap_gender = MetricFrame(\n",
    "    metrics={\"accuracy\": accuracy_score, \"selection_rate\": selection_rate},\n",
    "    y_true=y_test_pmute,\n",
    "    y_pred=y_pred_muted,\n",
    "    sensitive_features=df.loc[X_test_pmute.index, \"Gender_group\"]\n",
    ")\n",
    "print(\"\\nFairness metrics by Gender (ProxyMute SHAP):\")\n",
    "print(mf_shap_gender.by_group)\n",
    "\n",
    "# Age\n",
    "mf_shap_age = MetricFrame(\n",
    "    metrics={\"accuracy\": accuracy_score, \"selection_rate\": selection_rate},\n",
    "    y_true=y_test_pmute,\n",
    "    y_pred=y_pred_muted,\n",
    "    sensitive_features=df.loc[X_test_pmute.index, \"Age_encoded\"]\n",
    ")\n",
    "print(\"\\nFairness metrics by Age (ProxyMute SHAP):\")\n",
    "print(mf_shap_age.by_group)\n",
    "\n",
    "# EdLevel\n",
    "mf_shap_edlevel = MetricFrame(\n",
    "    metrics={\"accuracy\": accuracy_score, \"selection_rate\": selection_rate},\n",
    "    y_true=y_test_pmute,\n",
    "    y_pred=y_pred_muted,\n",
    "    sensitive_features=df.loc[X_test_pmute.index, \"EdLevel\"]\n",
    ")\n",
    "print(\"\\nFairness metrics by EdLevel (ProxyMute SHAP):\")\n",
    "print(mf_shap_edlevel.by_group)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6645038341122556\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.16      0.44      0.23      2576\n",
      "           1       0.90      0.69      0.79     19463\n",
      "\n",
      "    accuracy                           0.66     22039\n",
      "   macro avg       0.53      0.57      0.51     22039\n",
      "weighted avg       0.82      0.66      0.72     22039\n",
      "\n",
      "\n",
      "Accuracy by Gender group:\n",
      "Gender Man: 0.6562\n",
      "Gender NonBinary: 0.7399\n",
      "Gender Woman: 0.8012\n",
      "\n",
      "Accuracy by Age group:\n",
      "Age 0: 0.8737\n",
      "Age 1: 0.2825\n",
      "\n",
      "Accuracy by EdLevel group:\n",
      "EdLevel Master: 0.6631\n",
      "EdLevel NoHigherEd: 0.5967\n",
      "EdLevel Other: 0.5432\n",
      "EdLevel PhD: 0.595\n",
      "EdLevel Undergraduate: 0.7121\n"
     ]
    }
   ],
   "source": [
    "# --- 10. Consistent Utility Metrics ---\n",
    "evaluate_model(y_test_pmute, y_pred_muted)\n",
    "\n",
    "run_consistent_metrics(y_test_pmute, y_pred_muted, sensitive_col='Gender_group', label_name='Gender')\n",
    "run_consistent_metrics(y_test_pmute, y_pred_muted, sensitive_col='Age_encoded', label_name='Age')\n",
    "run_consistent_metrics(y_test_pmute, y_pred_muted, sensitive_col='EdLevel', label_name='EdLevel')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Selection Rates by Gender:\n",
      "0: 0.88\n",
      "2: 0.91\n",
      "1: 0.87\n",
      "\n",
      "Selection Rates by Age:\n",
      "<35: 0.90\n",
      ">35: 0.85\n",
      "\n",
      "Selection Rates by EdLevel:\n",
      "Master: 0.88\n",
      "Undergraduate: 0.90\n",
      "PhD: 0.90\n",
      "Other: 0.84\n",
      "NoHigherEd: 0.80\n",
      "\n",
      "Disparate Impact (2/0): 1.03\n",
      "\n",
      "Disparate Impact (1/0): 0.98\n",
      "\n",
      "Disparate Impact (>35/<35): 0.95\n",
      "\n",
      "Equal Opportunity by group:\n",
      "Man: TPR = 0.70\n",
      "NonBinary: TPR = 0.64\n",
      "Woman: TPR = 0.69\n",
      "\n",
      "Equal Opportunity by group:\n",
      "<35: TPR = 0.69\n",
      ">35: TPR = 0.70\n",
      "\n",
      "Equal Opportunity by group:\n",
      "Undergraduate: TPR = 0.70\n",
      "Master: TPR = 0.69\n",
      "PhD: TPR = 0.66\n",
      "Other: TPR = 0.69\n",
      "NoHigherEd: TPR = 0.68\n"
     ]
    }
   ],
   "source": [
    "# --- 11. Custom Fairness Metrics ---\n",
    "print_group_rates(df, 'Gender')\n",
    "print_group_rates(df, 'Age')\n",
    "print_group_rates(df, 'EdLevel')\n",
    "\n",
    "disparate_impact(df, 2, 0, 'Gender')  # Woman vs Man\n",
    "disparate_impact(df, 1, 0, 'Gender')  # NonBinary vs Man\n",
    "disparate_impact(df, '>35', '<35', 'Age')\n",
    "\n",
    "equal_opportunity(\n",
    "    y_test_pmute.reset_index(drop=True),\n",
    "    pd.Series(y_pred_muted),\n",
    "    [\"Man\", \"NonBinary\", \"Woman\"],  # group labels as strings\n",
    "    'Gender_group',  # string-based column in df\n",
    "    df.reset_index(drop=True)\n",
    ")\n",
    "\n",
    "\n",
    "equal_opportunity(\n",
    "    y_test_pmute.reset_index(drop=True),\n",
    "    pd.Series(y_pred_muted),\n",
    "    ['<35', '>35'],\n",
    "    'Age_group',\n",
    "    df.reset_index(drop=True)\n",
    ")\n",
    "\n",
    "# Equal Opportunity for Education Level\n",
    "equal_opportunity(\n",
    "    y_test_pmute.reset_index(drop=True),\n",
    "    pd.Series(y_pred_muted),\n",
    "    ['Undergraduate', 'Master', 'PhD', 'Other', 'NoHigherEd'],\n",
    "    'EdLevel',\n",
    "    df.reset_index(drop=True)\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b) Refined Proxy Mute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Initialize LIME Explainer ---\n",
    "explainer = LimeTabularExplainer(\n",
    "    training_data=X_train_pmute.values,\n",
    "    feature_names=X_train_pmute.columns.tolist(),\n",
    "    class_names=[\"Not Employed\", \"Employed\"],\n",
    "    mode=\"classification\",\n",
    "    discretize_continuous=False\n",
    ")\n",
    "\n",
    "# --- 2. Local Muting: Top 2 Features per Instance ---\n",
    "X_test_localmute_muted = X_test_pmute.copy()\n",
    "\n",
    "for i in range(500):  # Apply LIME only to top 500 for speed\n",
    "    exp = explainer.explain_instance(\n",
    "        X_test_pmute.iloc[i].values,\n",
    "        lambda x: lr_model_base.predict_proba(pd.DataFrame(x, columns=X_train_pmute.columns)),\n",
    "        num_features=2\n",
    "    )\n",
    "    top_features = [f[0] for f in exp.as_list()]\n",
    "    \n",
    "    for f in top_features:\n",
    "        f_name = f.split('<')[0].split('>')[0].split('=')[0].strip()\n",
    "        if f_name in X_test_localmute_muted.columns:\n",
    "            col_idx = X_test_localmute_muted.columns.get_loc(f_name)\n",
    "            mean_val = X_train_pmute[f_name].mean()\n",
    "            col_dtype = X_test_localmute_muted.dtypes[f_name]\n",
    "            X_test_localmute_muted.iat[i, col_idx] = col_dtype.type(mean_val)\n",
    "\n",
    "# --- 3. Predict ---\n",
    "y_pred_lime_localmute = lr_model_base.predict(X_test_localmute_muted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Refined ProxyMute (LIME, Top 2 Features):\n",
      "Accuracy: 0.6049276282952947\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.16      0.58      0.25      2576\n",
      "           1       0.92      0.61      0.73     19463\n",
      "\n",
      "    accuracy                           0.60     22039\n",
      "   macro avg       0.54      0.59      0.49     22039\n",
      "weighted avg       0.83      0.60      0.68     22039\n",
      "\n",
      "\n",
      "Fairness metrics by Gender:\n",
      "              accuracy  selection_rate\n",
      "Gender_group                          \n",
      "Man           0.595223        0.574640\n",
      "NonBinary     0.625298        0.634845\n",
      "Woman         0.792360        0.811949\n",
      "\n",
      "Fairness metrics by Age:\n",
      "             accuracy  selection_rate\n",
      "Age_encoded                          \n",
      "0            0.710062        0.725300\n",
      "1            0.412926        0.333804\n",
      "\n",
      "Fairness metrics by EdLevel:\n",
      "               accuracy  selection_rate\n",
      "EdLevel                                \n",
      "Master         0.484334        0.438308\n",
      "NoHigherEd     0.315466        0.161687\n",
      "Other          0.296569        0.174326\n",
      "PhD            0.557012        0.500655\n",
      "Undergraduate  0.787884        0.830379\n"
     ]
    }
   ],
   "source": [
    "# --- 4. Performance Metrics ---\n",
    "print(\"Refined ProxyMute (LIME, Top 2 Features):\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test_pmute, y_pred_lime_localmute))\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test_pmute, y_pred_lime_localmute))\n",
    "\n",
    "# --- 5. Fairness: MetricFrame by Group ---\n",
    "for attr, label in [(\"Gender_group\", \"Gender\"), (\"Age_encoded\", \"Age\"), (\"EdLevel\", \"EdLevel\")]:\n",
    "    mf = MetricFrame(\n",
    "        metrics={\"accuracy\": accuracy_score, \"selection_rate\": selection_rate},\n",
    "        y_true=y_test_pmute,\n",
    "        y_pred=y_pred_lime_localmute,\n",
    "        sensitive_features=df.loc[X_test_pmute.index, attr]\n",
    "    )\n",
    "    print(f\"\\nFairness metrics by {label}:\")\n",
    "    print(mf.by_group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6049276282952947\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.16      0.58      0.25      2576\n",
      "           1       0.92      0.61      0.73     19463\n",
      "\n",
      "    accuracy                           0.60     22039\n",
      "   macro avg       0.54      0.59      0.49     22039\n",
      "weighted avg       0.83      0.60      0.68     22039\n",
      "\n",
      "\n",
      "Accuracy by Gender group:\n",
      "Gender Man: 0.5952\n",
      "Gender NonBinary: 0.6253\n",
      "Gender Woman: 0.7924\n",
      "\n",
      "Accuracy by Age group:\n",
      "Age 0: 0.7101\n",
      "Age 1: 0.4129\n",
      "\n",
      "Accuracy by EdLevel group:\n",
      "EdLevel Master: 0.4843\n",
      "EdLevel NoHigherEd: 0.3155\n",
      "EdLevel Other: 0.2966\n",
      "EdLevel PhD: 0.557\n",
      "EdLevel Undergraduate: 0.7879\n"
     ]
    }
   ],
   "source": [
    "# --- 6. Custom Evaluation ---\n",
    "evaluate_model(y_test_pmute, y_pred_lime_localmute)\n",
    "\n",
    "run_consistent_metrics(y_test_pmute, y_pred_lime_localmute, sensitive_col='Gender_group', label_name='Gender')\n",
    "run_consistent_metrics(y_test_pmute, y_pred_lime_localmute, sensitive_col='Age_encoded', label_name='Age')\n",
    "run_consistent_metrics(y_test_pmute, y_pred_lime_localmute, sensitive_col='EdLevel', label_name='EdLevel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Selection Rates by Gender:\n",
      "0: 0.88\n",
      "2: 0.91\n",
      "1: 0.87\n",
      "\n",
      "Selection Rates by Age:\n",
      "<35: 0.90\n",
      ">35: 0.85\n",
      "\n",
      "Selection Rates by EdLevel:\n",
      "Master: 0.88\n",
      "Undergraduate: 0.90\n",
      "PhD: 0.90\n",
      "Other: 0.84\n",
      "NoHigherEd: 0.80\n",
      "\n",
      "Disparate Impact (2/0): 1.03\n",
      "\n",
      "Disparate Impact (1/0): 0.98\n",
      "\n",
      "Disparate Impact (>35/<35): 0.95\n",
      "\n",
      "Equal Opportunity by group:\n",
      "Man: TPR = 0.00\n",
      "NonBinary: TPR = 0.00\n",
      "Woman: TPR = 0.00\n",
      "\n",
      "Equal Opportunity by group:\n",
      "<35: TPR = 0.61\n",
      ">35: TPR = 0.61\n",
      "\n",
      "Equal Opportunity by group:\n",
      "Undergraduate: TPR = 0.61\n",
      "Master: TPR = 0.61\n",
      "PhD: TPR = 0.59\n",
      "Other: TPR = 0.59\n",
      "NoHigherEd: TPR = 0.61\n"
     ]
    }
   ],
   "source": [
    "# --- 7. Custom Fairness Functions ---\n",
    "print_group_rates(df, 'Gender')\n",
    "print_group_rates(df, 'Age')\n",
    "print_group_rates(df, 'EdLevel')\n",
    "\n",
    "disparate_impact(df, 2, 0, 'Gender')\n",
    "disparate_impact(df, 1, 0, 'Gender')\n",
    "disparate_impact(df, '>35', '<35', 'Age')\n",
    "\n",
    "equal_opportunity(\n",
    "    y_test_pmute.reset_index(drop=True),\n",
    "    pd.Series(y_pred_lime_localmute),\n",
    "    [\"Man\", \"NonBinary\", \"Woman\"],\n",
    "    'Gender',\n",
    "    df.reset_index(drop=True)\n",
    ")\n",
    "\n",
    "# EO for Age\n",
    "equal_opportunity(\n",
    "    y_test_pmute.reset_index(drop=True),\n",
    "    pd.Series(y_pred_lime_localmute),\n",
    "    ['<35', '>35'],\n",
    "    'Age_group',\n",
    "    df.reset_index(drop=True)\n",
    ")\n",
    "\n",
    "# EO for EdLevel\n",
    "equal_opportunity(\n",
    "    y_test_pmute.reset_index(drop=True),\n",
    "    pd.Series(y_pred_lime_localmute),\n",
    "    ['Undergraduate', 'Master', 'PhD', 'Other', 'NoHigherEd'],\n",
    "    'EdLevel',\n",
    "    df.reset_index(drop=True)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
